%% This BibTeX bibliography file was created using BibDesk.
%% https://bibdesk.sourceforge.io/

%% Created for Evan Drake at 2023-12-06 16:17:11 -0800 


%% Saved with string encoding Unicode (UTF-8) 



@misc{ambroszkiewicz_grounding_2019,
	abstract = {It is a ubiquitous opinion among mathematicians that a real number is just a point in the line. If this rough definition is not enough, then a mathematician may provide a formal definition of the real numbers in the set theoretic and axiomatic fashion, i.e. via Cauchy sequences or Dedekind cuts, or as the collection of axioms characterizing exactly (up to isomorphism) the set of real numbers as the complete and totally ordered Archimedean field. Actually, the above notions of the real numbers are abstract and do not have a constructive grounding. Definition of Cauchy sequences, and equivalence classes of these sequences explicitly use the actual infinity. The same is for Dedekind cuts, where the set of rational numbers is used as actual infinity. Although there is no direct constructive grounding for these abstract notions, there are so called intuitions on which they are based. A rigorous approach to express these very intuition in a constructive way is proposed. It is based on the concept of the adjacency relation that seems to be a missing primitive concept in type theory. The approach corresponds to the intuitionistic view of Continuum proposed by Brouwer. The famous and controversial Brouwer Continuity Theorem is discussed on the basis of different principle than the Axiom of Continuity.},
	author = {Ambroszkiewicz, Stanislaw},
	date = {2019-07-11},
	date-added = {2023-11-13 10:29:55 -0800},
	date-modified = {2023-11-13 10:29:55 -0800},
	eprint = {1510.02787 [cs, math]},
	eprinttype = {arxiv},
	file = {Grounding in continuum .pdf:/Users/evandrake/Documents/thesis/papers/Grounding in continuum .pdf:application/pdf},
	langid = {english},
	number = {{arXiv}:1510.02787},
	publisher = {{arXiv}},
	title = {The grounding for Continuum},
	url = {http://arxiv.org/abs/1510.02787},
	urldate = {2023-09-18},
	bdsk-url-1 = {http://arxiv.org/abs/1510.02787}}

@article{bossens_learning_2019,
	abstract = {Increasingly, autonomous agents will be required to operate on long-term missions. This will create a demand for general intelligence because feedback from a human operator may be sparse and delayed, and because not all behaviours can be prescribed. Deep neural networks and reinforcement learning methods can be applied in such environments but their fixed updating routines imply an inductive bias in learning spatio-temporal patterns, meaning some environments will be unsolvable. To address this problem, this paper proposes active adaptive perception, the ability of an architecture to learn when and how to modify and selectively utilise its perception module. To achieve this, a generic architecture based on a self-modifying policy ({SMP}) is proposed, and implemented using Incremental Self-improvement with the Success Story Algorithm. The architecture contrasts to deep reinforcement learning systems which follow fixed training strategies and earlier {SMP} studies which for perception relied either entirely on the working memory or on untrainable active perception instructions. One computationally cheap and one more expensive implementation are presented and compared to {DRQN}, an off-policy deep reinforcement learner using experience replay and Incremental Self-improvement, an {SMP}, on various non-episodic partially observable mazes. The results show that the simple instruction set leads to emergent strategies to avoid detracting corridors and rooms, and that the expensive implementation allows selectively ignoring perception where it is inaccurate.},
	author = {Bossens, D. M. and Townsend, N. C. and Sobey, A. J.},
	date = {2019},
	date-added = {2023-11-13 10:29:55 -0800},
	date-modified = {2023-11-13 10:29:55 -0800},
	doi = {https://doi.org/10.1016/j.neunet.2019.03.006},
	file = {NEUNET_D_18_00187R3.pdf:/Users/evandrake/Zotero/storage/QMTZC98N/NEUNET_D_18_00187R3.pdf:application/pdf},
	issn = {0893-6080},
	journaltitle = {Neural Networks},
	keywords = {adaptive learning},
	pages = {30--49},
	title = {Learning to learn with active adaptive perception},
	url = {https://www.sciencedirect.com/science/article/pii/S0893608019300796},
	volume = {115},
	bdsk-url-1 = {https://www.sciencedirect.com/science/article/pii/S0893608019300796},
	bdsk-url-2 = {https://doi.org/10.1016/j.neunet.2019.03.006}}

@inproceedings{chan_1993,
	author = {Chan, Philip K and Stolfo, Salvatore J},
	booktitle = {Proceedings of the second international conference on \{Information\} and knowledge management - \{{CIKM}\} '93},
	date = {1993},
	date-added = {2023-11-13 10:29:55 -0800},
	date-modified = {2023-11-13 10:29:55 -0800},
	doi = {10.1145/170088.170160},
	file = {PDF:/Users/evandrake/Zotero/storage/SYT7UDNE/Chan and Stolfo - 1993 - Experiments on multistrategy learning by meta-lear.pdf:application/pdf},
	isbn = {978-0-89791-626-4},
	keywords = {multistrategy learning},
	location = {Washington, D.C., United States},
	pages = {314--323},
	publisher = {{ACM} Press},
	title = {Experiments on multistrategy learning by meta-learning},
	url = {http://portal.acm.org/citation.cfm?doid=170088.170160},
	urldate = {2023-09-20},
	bdsk-url-1 = {http://portal.acm.org/citation.cfm?doid=170088.170160},
	bdsk-url-2 = {https://doi.org/10.1145/170088.170160}}

@article{chen_towards_2020,
	abstract = {Automation of fixpoint reasoning has been extensively studied for various mathematical structures, logical formalisms, and computational domains, resulting in specialized fixpoint provers for heaps, for streams, for term algebras, for temporal properties, for program correctness, and for many other formal systems and inductive and coinductive properties. However, in spite of great theoretical and practical interest, there is no unified framework for automated fixpoint reasoning. Although several attempts have been made, there is no evidence that such a unified framework is possible, or practical. In this paper, we propose a candidate based on matching logic, a formalism recently shown to theoretically unify the above mentioned formal systems. Unfortunately, the (Knaster-Tarski) proof rule of matching logic, which enables inductive reasoning, is not syntax-driven. Worse, it can be applied at any step during a proof, making automation seem hopeless. Inspired by recent advances in automation of inductive proofs in separation logic, we propose an alternative proof system for matching logic, which is amenable for automation. We then discuss our implementation of it, which although not superior to specialized state-of-the-art automated provers for specific domains, we believe brings some evidence and hope that a unified framework for automated reasoning is not out of reach.},
	author = {Chen, Xiaohong and Trinh, Minh-Thai and Rodrigues, Nishant and Pe{\~n}a, Lucas and Ro{\c s}u, Grigore},
	date = {2020-11-13},
	date-added = {2023-11-13 10:29:55 -0800},
	date-modified = {2023-11-13 10:29:55 -0800},
	doi = {10.1145/3428229},
	file = {Full Text PDF:/Users/evandrake/Zotero/storage/94DB3TQP/Chen et al. - 2020 - Towards a unified proof framework for automated fi.pdf:application/pdf},
	issn = {2475-1421},
	issue = {{OOPSLA}},
	journaltitle = {Proceedings of the {ACM} on Programming Languages},
	langid = {english},
	pages = {1--29},
	shortjournal = {Proc. {ACM} Program. Lang.},
	title = {Towards a unified proof framework for automated fixpoint reasoning using matching logic},
	url = {https://dl.acm.org/doi/10.1145/3428229},
	urldate = {2023-09-18},
	volume = {4},
	bdsk-url-1 = {https://dl.acm.org/doi/10.1145/3428229},
	bdsk-url-2 = {https://doi.org/10.1145/3428229}}

@online{church_-calculus_nodate,
	author = {Church, Alonzo},
	date-added = {2023-11-13 10:29:55 -0800},
	date-modified = {2023-12-06 16:15:21 -0800},
	file = {Snapshot:/Users/evandrake/Zotero/storage/DTH9IZJ9/supplementD.html:text/html},
	title = {lambda-Calculus and Type Theory},
	url = {https://plato.stanford.edu/entries/church/supplementD.html},
	urldate = {2023-10-09},
	bdsk-url-1 = {https://plato.stanford.edu/entries/church/supplementD.html}}

@article{cox_introspective_1999,
	abstract = {A central problem in multistrategy learning systems is the selection and sequencing of machine learning algorithms for particular situations. This is typically done by the system designer who analyzes the learning task and implements the appropriate algorithm or sequence of algorithms for that task. We propose a solution to this problem which enables an {AI} system with a library of machine learning algorithms to select and sequence appropriate algorithms autonomously. Furthermore, instead of relying on the system designer or user to provide a learning goal or target concept to the learning system, our method enables the system to determine its learning goals based on analysis of its successes and failures at the performance task. The method involves three steps: Given a performance failure, the learner examines a trace of its reasoning prior to the failure to diagnose what went wrong (blame assignment); given the resultant explanation of the reasoning failure, the learner posts explicitly represented learning goals to change its background knowledge (deciding what to learn); and given a set of learning goals, the learner uses nonlinear planning techniques to assemble a sequence of machine learning algorithms, represented as planning operators, to achieve the learning goals (learning-strategy construction). In support of these operations, we define the types of reasoning failures, a taxonomy of failure causes, a second-order formalism to represent reasoning traces, a taxonomy of learning goals that specify desired change to the background knowledge of a system, and a declarative task-formalism representation of learning algorithms. We present the Meta-{AQUA} system, an implemented multistrategy learner that operates in the domain of story understanding. Extensive empirical evaluations of Meta-{AQUA} show that it performs significantly better in a deliberative, planful mode than in a reflexive mode in which learning goals are ablated and, furthermore, that the arbitrary ordering of learning algorithms can lead to worse performance than no learning at all. We conclude that explicit representation and sequencing of learning goals is necessary for avoiding negative interactions between learning algorithms that can lead to less effective learning.},
	annotation = {Cox and Ram have created the Meta-{AQUA} story analysis algorithm. In this extensive paper Cox outlines the motivations behind what meta learning algorithms do as well as defines what it is that they should do. Cox explains that meta learning should be a three part endeavor: the learner must be able to preform blame assignment; the learner must be able to decide what to learn and form a set of learning goals; finally the learner needs to be able to use these goals to formulate a strategy and create a learning a plan. This is a multi-strategy style of meta learning; as such the learning strategy will be a collection of learning algorithms that better achieve results for the current task. Meta-{AQUA} does just this it analysis the context of its task and retrospectively preforms a self referential analysis to select new learning strategies (new sets of algorithms from its library or tool box). This paper is integral in my understanding of multi-strategy learning one of the main pillars of meta learning.},
	author = {Cox, Michael T. and Ram, Ashwin},
	date = {1999-08-01},
	date-added = {2023-11-13 10:29:55 -0800},
	date-modified = {2023-11-13 10:29:55 -0800},
	doi = {10.1016/S0004-3702(99)00047-8},
	file = {Full Text:/Users/evandrake/Zotero/storage/R29QLFVG/Cox and Ram - 1999 - Introspective multistrategy learning On the const.pdf:application/pdf;ScienceDirect Snapshot:/Users/evandrake/Zotero/storage/9FUJZN3V/S0004370299000478.html:text/html},
	issn = {0004-3702},
	journaltitle = {Artificial Intelligence},
	keywords = {introspective, multistrategy learning},
	number = {1},
	pages = {1--55},
	shortjournal = {Artificial Intelligence},
	shorttitle = {Introspective multistrategy learning},
	title = {Introspective multistrategy learning: On the construction of learning strategies},
	url = {https://www.sciencedirect.com/science/article/pii/S0004370299000478},
	urldate = {2023-09-20},
	volume = {112},
	bdsk-url-1 = {https://www.sciencedirect.com/science/article/pii/S0004370299000478},
	bdsk-url-2 = {https://doi.org/10.1016/S0004-3702(99)00047-8}}

@article{curry_combinatory_1964,
	author = {Curry, Haskell B.},
	date = {1964},
	date-added = {2023-11-13 10:29:55 -0800},
	date-modified = {2023-11-13 10:29:55 -0800},
	doi = {10.1090/S0002-9904-1964-11245-3},
	file = {curry recursion.pdf:/Users/evandrake/Documents/thesis/papers/curry recursion.pdf:application/pdf},
	issn = {0273-0979, 1088-9485},
	journaltitle = {Bulletin of the American Mathematical Society},
	langid = {english},
	number = {6},
	pages = {814--817},
	shortjournal = {Bull. Amer. Math. Soc.},
	title = {Combinatory recursive objects of all finite types},
	url = {https://www.ams.org/bull/1964-70-06/S0002-9904-1964-11245-3/},
	urldate = {2023-09-18},
	volume = {70},
	bdsk-url-1 = {https://www.ams.org/bull/1964-70-06/S0002-9904-1964-11245-3/},
	bdsk-url-2 = {https://doi.org/10.1090/S0002-9904-1964-11245-3}}

@incollection{deutsch_alonzo_2023,
	author = {Deutsch, Harry and Marshall, Oliver},
	booktitle = {The Stanford Encyclopedia of Philosophy},
	date = {2023},
	date-added = {2023-11-13 10:29:55 -0800},
	date-modified = {2023-11-13 10:29:55 -0800},
	edition = {Winter 2023},
	editor = {Zalta, Edward N. and Nodelman, Uri},
	file = {SEP - Snapshot:/Users/evandrake/Zotero/storage/BHDZQXMS/index.html:text/html},
	publisher = {Metaphysics Research Lab, Stanford University},
	title = {Alonzo Church},
	url = {https://plato.stanford.edu/archives/win2023/entries/church/},
	bdsk-url-1 = {https://plato.stanford.edu/archives/win2023/entries/church/}}

@misc{finn_model-agnostic_2017,
	abstract = {We propose an algorithm for meta-learning that is model-agnostic, in the sense that it is compatible with any model trained with gradient descent and applicable to a variety of different learning problems, including classification, regression, and reinforcement learning. The goal of meta-learning is to train a model on a variety of learning tasks, such that it can solve new learning tasks using only a small number of training samples. In our approach, the parameters of the model are explicitly trained such that a small number of gradient steps with a small amount of training data from a new task will produce good generalization performance on that task. In effect, our method trains the model to be easy to fine-tune. We demonstrate that this approach leads to state-of-the-art performance on two few-shot image classification benchmarks, produces good results on few-shot regression, and accelerates fine-tuning for policy gradient reinforcement learning with neural network policies.},
	annotation = {Finn et al. describes in fine grain detail an algorithm for an adaptive few-shot learning model. This model is a meta model that can absorb any other learning model given it has certain requirements. These requirements are translated into the parameters of the learning model and the meta-learning algorithm uses them to make adjustments according to the current task. This paper broadens my pool of adaptive meta-learning models and serves as a literature standard for adaptive learning styles.},
	author = {Finn, Chelsea and Abbeel, Pieter and Levine, Sergey},
	date = {2017-07-18},
	date-added = {2023-11-13 10:29:55 -0800},
	date-modified = {2023-11-13 10:29:55 -0800},
	eprint = {1703.03400 [cs]},
	eprinttype = {arxiv},
	file = {arXiv.org Snapshot:/Users/evandrake/Zotero/storage/TCAKTTBZ/1703.html:text/html;Full Text PDF:/Users/evandrake/Zotero/storage/9LCE67MI/Finn et al. - 2017 - Model-Agnostic Meta-Learning for Fast Adaptation o.pdf:application/pdf},
	keywords = {adaptive learning},
	number = {{arXiv}:1703.03400},
	publisher = {{arXiv}},
	title = {Model-Agnostic Meta-Learning for Fast Adaptation of Deep Networks},
	url = {http://arxiv.org/abs/1703.03400},
	urldate = {2023-09-25},
	bdsk-url-1 = {http://arxiv.org/abs/1703.03400}}

@article{gabor_self-replication_2022,
	abstract = {Abstract
            A key element of biological structures is self-replication. Neural networks are the prime structure used for the emergent construction of complex behavior in computers. We analyze how various network types lend themselves to self-replication. Backpropagation turns out to be the natural way to navigate the space of network weights and allows non-trivial self-replicators to arise naturally. We perform an in-depth analysis to show the self-replicators' robustness to noise. We then introduce artificial chemistry environments consisting of several neural networks and examine their emergent behavior. In extension to this work's previous version (Gabor et al., 2019), we provide an extensive analysis of the occurrence of fixpoint weight configurations within the weight space and an approximation of their respective attractor basins.},
	author = {Gabor, Thomas and Illium, Steffen and Zorn, Maximilian and Lenta, Cristian and Mattausch, Andy and Belzner, Lenz and Linnhoff-Popien, Claudia},
	date = {2022-06-28},
	date-added = {2023-11-13 10:29:55 -0800},
	date-modified = {2023-11-13 10:29:55 -0800},
	doi = {10.1162/artl_a_00359},
	file = {Gabor-2022.pdf:/Users/evandrake/Documents/Thesis/thesis_sup/papers/Gabor-2022.pdf:application/pdf},
	issn = {1064-5462, 1530-9185},
	journaltitle = {Artificial Life},
	keywords = {self-reference},
	langid = {english},
	number = {2},
	pages = {205--223},
	title = {Self-Replication in Neural Networks},
	url = {https://direct.mit.edu/artl/article/28/2/205/111793/Self-Replication-in-Neural-Networks},
	urldate = {2023-09-18},
	volume = {28},
	bdsk-url-1 = {https://direct.mit.edu/artl/article/28/2/205/111793/Self-Replication-in-Neural-Networks},
	bdsk-url-2 = {https://doi.org/10.1162/artl_a_00359}}

@article{gill_domain-specific_2014,
	author = {Gill, Andy},
	date = {2014-04-15},
	date-added = {2023-11-13 10:29:55 -0800},
	date-modified = {2023-11-13 10:29:55 -0800},
	file = {2611429.2617811:/Users/evandrake/Zotero/storage/FIDCI3LC/2611429.2617811:application/pdf},
	journaltitle = {University of Kansas},
	title = {Domain-specific Languages and Code Synthesis Using Haskell}}

@article{giloi_konrad_1997,
	author = {Giloi, W.K.},
	date = {1997-06},
	date-added = {2023-11-13 10:29:55 -0800},
	date-modified = {2023-11-13 10:29:55 -0800},
	doi = {10.1109/85.586068},
	file = {Zuse_Plan_Kalkul.pdf:/Users/evandrake/Documents/thesis/papers/Zuse_Plan_Kalkul.pdf:application/pdf},
	issn = {10586180},
	journaltitle = {{IEEE} Annals of the History of Computing},
	langid = {english},
	number = {2},
	pages = {17--24},
	shortjournal = {{IEEE} Annals Hist. Comput.},
	shorttitle = {Konrad Zuse's Plankalk{\"u}l},
	title = {Konrad Zuse's Plankalk{\"u}l: the first high-level, "non von Neumann" programming language},
	url = {http://ieeexplore.ieee.org/document/586068/},
	urldate = {2023-09-18},
	volume = {19},
	bdsk-url-1 = {http://ieeexplore.ieee.org/document/586068/},
	bdsk-url-2 = {https://doi.org/10.1109/85.586068}}

@misc{gopalakrishnan_unsupervised_2022,
	abstract = {The discovery of reusable sub-routines simplifies decision-making and planning in complex reinforcement learning problems. Previous approaches propose to learn such temporal abstractions in a purely unsupervised fashion through observing state-action trajectories gathered from executing a policy. However, a current limitation is that they process each trajectory in an entirely sequential manner, which prevents them from revising earlier decisions about sub-routine boundary points in light of new incoming information. In this work we propose {SloTTAr}, a fully parallel approach that integrates sequence processing Transformers with a Slot Attention module and adaptive computation for learning about the number of such sub-routines in an unsupervised fashion. We demonstrate how {SloTTAr} is capable of outperforming strong baselines in terms of boundary point discovery, even for sequences containing variable amounts of sub-routines, while being up to 7x faster to train on existing benchmarks.},
	author = {Gopalakrishnan, Anand and Irie, Kazuki and Schmidhuber, J{\"u}rgen and van Steenkiste, Sjoerd},
	date = {2022-11-22},
	date-added = {2023-11-13 10:29:55 -0800},
	date-modified = {2023-11-13 10:29:55 -0800},
	eprint = {2203.13573 [cs]},
	eprinttype = {arxiv},
	file = {arXiv.org Snapshot:/Users/evandrake/Zotero/storage/UBMEF9CM/2203.html:text/html;Full Text PDF:/Users/evandrake/Zotero/storage/9RF8KJX2/Gopalakrishnan et al. - 2022 - Unsupervised Learning of Temporal Abstractions wit.pdf:application/pdf},
	number = {{arXiv}:2203.13573},
	publisher = {{arXiv}},
	title = {Unsupervised Learning of Temporal Abstractions with Slot-based Transformers},
	url = {http://arxiv.org/abs/2203.13573},
	urldate = {2023-09-24},
	bdsk-url-1 = {http://arxiv.org/abs/2203.13573}}

@article{godel_formally_1931,
	author = {G{\"o}del, Kurt},
	date = {1931},
	date-added = {2023-11-13 10:29:55 -0800},
	date-modified = {2023-11-13 10:29:55 -0800},
	file = {G{\"o}del - On Formally Undecidable Propositions of Principia .pdf:/Users/evandrake/Zotero/storage/UW3SSUFZ/G{\"o}del - On Formally Undecidable Propositions of Principia .pdf:application/pdf;Godel-1931.pdf:/Users/evandrake/Documents/thesis/papers/Godel-1931.pdf:application/pdf},
	keywords = {proof theory},
	langid = {english},
	title = {On Formally Undecidable Propositions of Principia Mathematica And Related Systems},
	translator = {B, Meltzer}}

@article{guo_comprehensive_2023,
	abstract = {The increasing significance of theorem proving-based formalization in mathematics and computer science highlights the necessity for formalizing foundational mathematical theories. In this work, we employ the Coq interactive theorem prover to methodically formalize the language, semantics, and syntax of propositional logic, a fundamental aspect of mathematical reasoning and proof construction. We construct four Hilbert-style axiom systems and a natural deduction system for propositional logic, and establish their equivalences through meticulous proofs. Moreover, we provide formal proofs for essential meta-theorems in propositional logic, including the Deduction Theorem, Soundness Theorem, Completeness Theorem, and Compactness Theorem. Importantly, we present an exhaustive formal proof of the Completeness Theorem in this paper. To bolster the proof of the Completeness Theorem, we also formalize concepts related to mappings and countability, and deliver a formal proof of the Cantor--Bernstein--Schr{\"o}der theorem. Additionally, we devise automated Coq tactics explicitly designed for the propositional logic inference system delineated in this study, enabling the automatic verification of all tautologies, all internal theorems, and the majority of syntactic and semantic inferences within the system. This research contributes a versatile and reusable Coq library for propositional logic, presenting a solid foundation for numerous applications in mathematics, such as the accurate expression and verification of properties in software programs and digital circuits. This work holds particular importance in the domains of mathematical formalization, verification of software and hardware security, and in enhancing comprehension of the principles of logical reasoning.},
	author = {Guo, Dakai and Link to external site, this link will open in a new window and Yu, Wensheng},
	date = {2023},
	date-added = {2023-11-13 10:29:55 -0800},
	date-modified = {2023-11-13 10:29:55 -0800},
	doi = {10.3390/math11112504},
	file = {Full Text PDF:/Users/evandrake/Zotero/storage/EB8WJWIB/Guo et al. - 2023 - A Comprehensive Formalization of Propositional Log.pdf:application/pdf},
	journaltitle = {Mathematics},
	note = {Num Pages: 2504 Place: Basel, Switzerland Publisher: {MDPI} {AG}},
	number = {11},
	pages = {2504},
	rights = {{\copyright} 2023 by the authors. Licensee {MDPI}, Basel, Switzerland. This article is an open access article distributed under the terms and conditions of the Creative Commons Attribution ({CC} {BY}) license (https://creativecommons.org/licenses/by/4.0/). Notwithstanding the {ProQuest} Terms and Conditions, you may use this content in accordance with the terms of the License.},
	shorttitle = {A Comprehensive Formalization of Propositional Logic in Coq},
	title = {A Comprehensive Formalization of Propositional Logic in Coq: Deduction Systems, Meta-Theorems, and Automation Tactics},
	url = {https://www.proquest.com/docview/2824017491/abstract/7553220F76814F76PQ/2},
	urldate = {2023-09-18},
	volume = {11},
	bdsk-url-1 = {https://www.proquest.com/docview/2824017491/abstract/7553220F76814F76PQ/2},
	bdsk-url-2 = {https://doi.org/10.3390/math11112504}}

@article{hochreiter_long_1997,
	author = {Hochreiter, Sepp and Schmidhuber, J{\"u}rgen},
	date = {1997},
	date-added = {2023-11-13 10:29:55 -0800},
	date-modified = {2023-11-13 10:29:55 -0800},
	file = {lstm.pdf:/Users/evandrake/Zotero/storage/QX6S7MFQ/lstm.pdf:application/pdf},
	journaltitle = {Neural computation},
	note = {Publisher: {MIT} press},
	number = {8},
	pages = {1735--1780},
	title = {Long short-term memory},
	volume = {9}}

@article{hospedales_meta-learning_2022,
	abstract = {The field of meta-learning, or learning-to-learn, has seen a dramatic rise in interest in recent years. Contrary to conventional approaches to {AI} where tasks are solved from scratch using a fixed learning algorithm, meta-learning aims to improve the learning algorithm itself, given the experience of multiple learning episodes. This paradigm provides an opportunity to tackle many conventional challenges of deep learning, including data and computation bottlenecks, as well as generalization. This survey describes the contemporary meta-learning landscape. We first discuss definitions of meta-learning and position it with respect to related fields, such as transfer learning and hyperparameter optimization. We then propose a new taxonomy that provides a more comprehensive breakdown of the space of meta-learning methods today. We survey promising applications and successes of meta-learning such as few-shot learning and reinforcement learning. Finally, we discuss outstanding challenges and promising areas for future research.},
	author = {Hospedales, Timothy and Antoniou, Antreas and Micaelli, Paul and Storkey, Amos},
	date = {2022-09},
	date-added = {2023-11-13 10:29:55 -0800},
	date-modified = {2023-11-13 10:29:55 -0800},
	doi = {10.1109/TPAMI.2021.3079209},
	file = {IEEE Xplore Full Text PDF:/Users/evandrake/Zotero/storage/7CTXRXW6/Hospedales et al. - 2022 - Meta-Learning in Neural Networks A Survey.pdf:application/pdf},
	issn = {1939-3539},
	journaltitle = {{IEEE} Transactions on Pattern Analysis and Machine Intelligence},
	keywords = {survey},
	note = {Conference Name: {IEEE} Transactions on Pattern Analysis and Machine Intelligence},
	number = {9},
	pages = {5149--5169},
	shorttitle = {Meta-Learning in Neural Networks},
	title = {Meta-Learning in Neural Networks: A Survey},
	volume = {44},
	bdsk-url-1 = {https://doi.org/10.1109/TPAMI.2021.3079209}}

@book{hutter_2019,
	author = {Hutter, F},
	date = {2019},
	date-added = {2023-11-13 10:29:55 -0800},
	date-modified = {2023-11-13 10:29:55 -0800},
	doi = {10.1007/978-3-030-05318-5},
	editor = {Hutter, Frank and Kotthoff, Lars and Vanschoren, Joaquin},
	file = {PDF:/Users/evandrake/Zotero/storage/NYSII6MA/Hutter et al. - 2019 - Automated Machine Learning Methods, Systems, Chal.pdf:application/pdf},
	isbn = {978-3-030-05317-8 978-3-030-05318-5},
	keywords = {survey},
	location = {Cham},
	note = {Series Title: The \{Springer\} \{Series\} on \{Challenges\} in \{Machine\} \{Learning\}},
	publisher = {Springer International Publishing},
	shorttitle = {Automated \{Machine\} \{Learning\}},
	title = {Automated \{Machine\} \{Learning\}: \{Methods\}, \{Systems\}, \{Challenges\}},
	url = {http://link.springer.com/10.1007/978-3-030-05318-5},
	urldate = {2023-09-20},
	bdsk-url-1 = {http://link.springer.com/10.1007/978-3-030-05318-5},
	bdsk-url-2 = {https://doi.org/10.1007/978-3-030-05318-5}}

@inproceedings{irie_going_2021,
	abstract = {Transformers with linearised attention (''linear Transformers'') have demonstrated the practical scalability and effectiveness of outer product-based Fast Weight Programmers ({FWPs}) from the '90s. However, the original {FWP} formulation is more general than the one of linear Transformers: a slow neural network ({NN}) continually reprograms the weights of a fast {NN} with arbitrary architecture. In existing linear Transformers, both {NNs} are feedforward and consist of a single layer. Here we explore new variations by adding recurrence to the slow and fast nets. We evaluate our novel recurrent {FWPs} ({RFWPs}) on two synthetic algorithmic tasks (code execution and sequential {ListOps}), Wikitext-103 language models, and on the Atari 2600 2D game environment. Our models exhibit properties of Transformers and {RNNs}. In the reinforcement learning setting, we report large improvements over {LSTM} in several Atari games. Our code is public.},
	author = {Irie, Kazuki and Schlag, Imanol and Csord{\'a}s, R{\'o}bert and Schmidhuber, J{\"u}rgen},
	booktitle = {Advances in Neural Information Processing Systems},
	date = {2021},
	date-added = {2023-11-13 10:29:55 -0800},
	date-modified = {2023-11-13 10:29:55 -0800},
	file = {Full Text PDF:/Users/evandrake/Zotero/storage/959RHCK8/Irie et al. - 2021 - Going Beyond Linear Transformers with Recurrent Fa.pdf:application/pdf},
	keywords = {fast weight programmer, adaptive learning},
	pages = {7703--7717},
	publisher = {Curran Associates, Inc.},
	title = {Going Beyond Linear Transformers with Recurrent Fast Weight Programmers},
	url = {https://proceedings.neurips.cc/paper_files/paper/2021/hash/3f9e3767ef3b10a0de4c256d7ef9805d-Abstract.html},
	urldate = {2023-09-24},
	volume = {34},
	bdsk-url-1 = {https://proceedings.neurips.cc/paper_files/paper/2021/hash/3f9e3767ef3b10a0de4c256d7ef9805d-Abstract.html}}

@misc{irie_modern_2022,
	abstract = {The weight matrix ({WM}) of a neural network ({NN}) is its program. The programs of many traditional {NNs} are learned through gradient descent in some error function, then remain fixed. The {WM} of a self-referential {NN}, however, can keep rapidly modifying all of itself during runtime. In principle, such {NNs} can meta-learn to learn, and meta-meta-learn to meta-learn to learn, and so on, in the sense of recursive self-improvement. While {NN} architectures potentially capable of implementing such behaviour have been proposed since the '90s, there have been few if any practical studies. Here we revisit such {NNs}, building upon recent successes of fast weight programmers and closely related linear Transformers. We propose a scalable self-referential {WM} ({SRWM}) that learns to use outer products and the delta update rule to modify itself. We evaluate our {SRWM} in supervised few-shot learning and in multi-task reinforcement learning with procedurally generated game environments. Our experiments demonstrate both practical applicability and competitive performance of the proposed {SRWM}. Our code is public.},
	annotation = {Irie takes the introspective abstract scheme that Schmidhuber derived and expands on it. Utilizing rising research in fast weight programmers the authors gives an instructional approach to how the introspective phase works. In an effort to make obvious the claim that ``this {SRWM} can replace any weighted matrix'' the authors illustrate high level schemes of the machine in the form of black-box diagrams. The authors of this paper emphasize the concept of domain specific (and general) adaption of the algorithm. through standard training practice the authors demonstrate the effectiveness of introspective adaption in which the algorithm can change the way it learns over time by realizing certain patterns. This paper serves as the latest example of a meta learning algorithm that is both possible and probable to implement in the real world.},
	author = {Irie, Kazuki and Schlag, Imanol and Csord{\'a}s, R{\'o}bert and Schmidhuber, J{\"u}rgen},
	date = {2022-06-17},
	date-added = {2023-11-13 10:29:55 -0800},
	date-modified = {2023-11-13 10:29:55 -0800},
	doi = {10.48550/arXiv.2202.05780},
	eprint = {2202.05780 [cs]},
	eprinttype = {arxiv},
	file = {arXiv Fulltext PDF:/Users/evandrake/Zotero/storage/KTLQIIBZ/Irie et al. - 2022 - A Modern Self-Referential Weight Matrix That Learn.pdf:application/pdf;arXiv.org Snapshot:/Users/evandrake/Zotero/storage/2UEIXWYY/2202.html:text/html},
	keywords = {self-reference, self modification, fast weight programmer, introspective, adaptive learning},
	number = {{arXiv}:2202.05780},
	publisher = {{arXiv}},
	title = {A Modern Self-Referential Weight Matrix That Learns to Modify Itself},
	url = {http://arxiv.org/abs/2202.05780},
	urldate = {2023-10-23},
	bdsk-url-1 = {http://arxiv.org/abs/2202.05780},
	bdsk-url-2 = {https://doi.org/10.48550/arXiv.2202.05780}}

@article{jiang_thor_nodate,
	abstract = {In theorem proving, the task of selecting useful premises from a large library to unlock the proof of a given conjecture is crucially important. This presents a challenge for all theorem provers, especially the ones based on language models, due to their relative inability to reason over huge volumes of premises in text form. This paper introduces Thor, a framework integrating language models and automated theorem provers to overcome this difficulty. In Thor, a class of methods called hammers that leverage the power of automated theorem provers are used for premise selection, while all other tasks are designated to language models. Thor increases a language model's success rate on the {PISA} dataset from 39\% to 57\%, while solving 8.2\% of problems neither language models nor automated theorem provers are able to solve on their own. Furthermore, with a significantly smaller computational budget, Thor can achieve a success rate on the {MiniF}2F dataset that is on par with the best existing methods. Thor can be instantiated for the majority of popular interactive theorem provers via a straightforward protocol we provide.},
	author = {Jiang, Albert Q and Li, Wenda and Tworkowski, Szymon and Czechowski, Konrad and Odrzyg{\'o}zdz, Tomasz and Mi{\l}os, Piotr and Wu, Yuhuai and Jamnik, Mateja},
	date-added = {2023-11-13 10:29:55 -0800},
	date-modified = {2023-11-13 10:29:55 -0800},
	file = {Jiang et al. - Thor Wielding Hammers to Integrate Language Model.pdf:/Users/evandrake/Zotero/storage/QE8LDQGI/Jiang et al. - Thor Wielding Hammers to Integrate Language Model.pdf:application/pdf},
	langid = {english},
	title = {Thor: Wielding Hammers to Integrate Language Models and Automated Theorem Provers}}

@article{lei_machine_2023,
	abstract = {An important development in geometric algebra in recent years is the new system known as point geometry, which treats points as direct objects of operations and considerably simplifies the process of geometric reasoning. In this paper, we provide a complete formal description of the point geometry theory architecture and give a rigorous and reliable formal verification of the point geometry theory based on the theorem prover Coq. Simultaneously, a series of tactics are also designed to assist in the proof of geometric propositions. Based on the theoretical architecture and proof tactics, a universal and scalable interactive point geometry machine proof system, {PointGeo}, is built. In this system, any arbitrary point-geometry-solvable geometric statement may be proven, along with readable information about the solution's procedure. Additionally, users may augment the rule base by adding trustworthy rules as needed for certain issues. The implementation of the system expands the library of Coq resources on geometric algebra, which will become a significant research foundation for the fields of geometric algebra, computer science, mathematics education, and other related fields.},
	author = {Lei, Siran and Guan, Hao and Jiang, Jianguo and Zou, Yu and Rao, Yongsheng},
	date = {2023-06-18},
	date-added = {2023-11-13 10:29:55 -0800},
	date-modified = {2023-11-13 10:29:55 -0800},
	doi = {10.3390/math11122757},
	file = {out.pdf:/Users/evandrake/Downloads/out.pdf:application/pdf},
	issn = {2227-7390},
	journaltitle = {Mathematics},
	langid = {english},
	number = {12},
	pages = {2757},
	shortjournal = {Mathematics},
	title = {A Machine Proof System of Point Geometry Based on Coq},
	url = {https://www.mdpi.com/2227-7390/11/12/2757},
	urldate = {2023-09-18},
	volume = {11},
	bdsk-url-1 = {https://www.mdpi.com/2227-7390/11/12/2757},
	bdsk-url-2 = {https://doi.org/10.3390/math11122757}}

@misc{li_automated_2022,
	abstract = {We present a versatile automated theorem proving framework capable of automated discovery, simplification and proofs of inner and outer bounds in network information theory, deduction of properties of information-theoretic quantities (e.g. Wyner and G{\textbackslash}'acs-K{\textbackslash}"orner common information), and discovery of non-Shannon-type inequalities, under a unified framework. Our implementation successfully generated proofs for 32 out of 56 theorems in Chapters 1-14 of the book Network Information Theory by El Gamal and Kim. Our framework is based on the concept of existential information inequalities, which provides an axiomatic framework for a wide range of problems in information theory.},
	author = {Li, Cheuk Ting},
	date = {2022-07-11},
	date-added = {2023-11-13 10:29:55 -0800},
	date-modified = {2023-11-13 10:29:55 -0800},
	eprint = {2101.12370 [cs, math]},
	eprinttype = {arxiv},
	file = {arXiv.org Snapshot:/Users/evandrake/Zotero/storage/NAZFYXAY/2101.html:text/html;Full Text PDF:/Users/evandrake/Zotero/storage/PMGV7Y9H/Li - 2022 - An Automated Theorem Proving Framework for Informa.pdf:application/pdf},
	number = {{arXiv}:2101.12370},
	publisher = {{arXiv}},
	title = {An Automated Theorem Proving Framework for Information-Theoretic Results},
	url = {http://arxiv.org/abs/2101.12370},
	urldate = {2023-09-18},
	bdsk-url-1 = {http://arxiv.org/abs/2101.12370}}

@misc{machowczyk_graph_2023,
	abstract = {Given graphs as input, Graph Neural Networks ({GNNs}) support the inference of nodes, edges, attributes, or graph properties. Graph Rewriting investigates the rule-based manipulation of graphs to model complex graph transformations. We propose that, therefore, (i) graph rewriting subsumes {GNNs} and could serve as formal model to study and compare them, and (ii) the representation of {GNNs} as graph rewrite systems can help to design and analyse {GNNs}, their architectures and algorithms. Hence we propose Graph Rewriting Neural Networks ({GReNN}) as both novel semantic foundation and engineering discipline for {GNNs}. We develop a case study reminiscent of a Message Passing Neural Network realised as a Groove graph rewriting model and explore its incremental operation in response to dynamic updates.},
	author = {Machowczyk, Adam and Heckel, Reiko},
	date = {2023-05-29},
	date-added = {2023-11-13 10:29:55 -0800},
	date-modified = {2023-11-13 10:29:55 -0800},
	eprint = {2305.18632 [cs]},
	eprinttype = {arxiv},
	file = {Machowczyk and Heckel - 2023 - Graph Rewriting for Graph Neural Networks.pdf:/Users/evandrake/Zotero/storage/RPIWI6X5/Machowczyk and Heckel - 2023 - Graph Rewriting for Graph Neural Networks.pdf:application/pdf},
	number = {{arXiv}:2305.18632},
	title = {Graph Rewriting for Graph Neural Networks},
	url = {http://arxiv.org/abs/2305.18632},
	urldate = {2023-09-18},
	bdsk-url-1 = {http://arxiv.org/abs/2305.18632}}

@incollection{goos_functional_1991,
	abstract = {We develop a calculus for lazy functional programming based on recursion operators associated with data type de nitions. For these operators we derive various algebraic laws that are useful in deriving and manipulating programs. We shall show that all example functions in Bird and Wadler's {\textbackslash}Introduction to Functional Programming" can be expressed using these operators.},
	author = {Meijer, Erik and Fokkinga, Maarten and Paterson, Ross},
	booktitle = {Functional Programming Languages and Computer Architecture},
	date = {1991},
	date-added = {2023-11-13 10:29:55 -0800},
	date-modified = {2023-11-13 10:29:55 -0800},
	doi = {10.1007/3540543961_7},
	editor = {Hughes, John},
	editorb = {Goos, Gerhard and Hartmanis, Juris},
	editorbtype = {redactor},
	file = {Meijer et al. - 1991 - Functional programming with bananas, lenses, envel.pdf:/Users/evandrake/Zotero/storage/33ARA7WG/Meijer et al. - 1991 - Functional programming with bananas, lenses, envel.pdf:application/pdf},
	isbn = {978-3-540-54396-1 978-3-540-47599-6},
	langid = {english},
	location = {Berlin, Heidelberg},
	note = {Series Title: Lecture Notes in Computer Science},
	pages = {124--144},
	publisher = {Springer Berlin Heidelberg},
	title = {Functional programming with bananas, lenses, envelopes and barbed wire},
	url = {http://link.springer.com/10.1007/3540543961_7},
	urldate = {2023-10-12},
	volume = {523},
	bdsk-url-1 = {http://link.springer.com/10.1007/3540543961_7},
	bdsk-url-2 = {https://doi.org/10.1007/3540543961_7}}

@misc{miconi_backpropamine_2020,
	abstract = {The impressive lifelong learning in animal brains is primarily enabled by plastic changes in synaptic connectivity. Importantly, these changes are not passive, but are actively controlled by neuromodulation, which is itself under the control of the brain. The resulting self-modifying abilities of the brain play an important role in learning and adaptation, and are a major basis for biological reinforcement learning. Here we show for the first time that artificial neural networks with such neuromodulated plasticity can be trained with gradient descent. Extending previous work on differentiable Hebbian plasticity, we propose a differentiable formulation for the neuromodulation of plasticity. We show that neuromodulated plasticity improves the performance of neural networks on both reinforcement learning and supervised learning tasks. In one task, neuromodulated plastic {LSTMs} with millions of parameters outperform standard {LSTMs} on a benchmark language modeling task (controlling for the number of parameters). We conclude that differentiable neuromodulation of plasticity offers a powerful new framework for training neural networks.},
	annotation = {Miconi illustrates meta learning from a biological basis; examining their neuromodulative algorithm Backpropamine. An algorithm based on the meta learning of the human brain. The idea is taken from how humans learn; a human does not learn based on an immutable algorithm. Humans learn by mapping experience to action in a retro active manner. Miconi gives an algorithm for neuromodulation based on Hebbian plasticity; the weight of the connection i{\textasciitilde}j determines the adaption potential of neurons i and j. Miconi's Backpropamine is based on utilizing gradient decent to optimize the hebbian plasticity of each neural connection as a function of time. This paper gives an approach to meta learning that is not derived from fast weighted matrices. The idea of retro active evaluation can be implemented to assist in implementing a Godel machines' proofing mechanism.},
	author = {Miconi, Thomas and Rawal, Aditya and Clune, Jeff and Stanley, Kenneth O.},
	date = {2020-02-24},
	date-added = {2023-11-13 10:29:55 -0800},
	date-modified = {2023-11-13 10:29:55 -0800},
	eprint = {2002.10585 [cs]},
	eprinttype = {arxiv},
	file = {arXiv.org Snapshot:/Users/evandrake/Zotero/storage/YYKFI7AT/2002.html:text/html;Full Text PDF:/Users/evandrake/Zotero/storage/BMW46WY2/Miconi et al. - 2020 - Backpropamine training self-modifying neural netw.pdf:application/pdf},
	keywords = {self-reference, self modification, neuro science / brain inspired},
	number = {{arXiv}:2002.10585},
	publisher = {{arXiv}},
	shorttitle = {Backpropamine},
	title = {Backpropamine: training self-modifying neural networks with differentiable neuromodulated plasticity},
	url = {2},
	urldate = {2023-09-20},
	bdsk-url-1 = {2}}

@misc{munkhdalai_metalearning_2018,
	abstract = {We unify recent neural approaches to one-shot learning with older ideas of associative memory in a model for metalearning. Our model learns jointly to represent data and to bind class labels to representations in a single shot. It builds representations via slow weights, learned across tasks through {SGD}, while fast weights constructed by a Hebbian learning rule implement one-shot binding for each new task. On the Omniglot, Mini-{ImageNet}, and Penn Treebank one-shot learning benchmarks, our model achieves state-of-the-art results.},
	author = {Munkhdalai, Tsendsuren and Trischler, Adam},
	date = {2018-07-12},
	date-added = {2023-11-13 10:29:55 -0800},
	date-modified = {2023-11-13 10:29:55 -0800},
	eprint = {1807.05076 [cs, stat]},
	eprinttype = {arxiv},
	file = {arXiv.org Snapshot:/Users/evandrake/Zotero/storage/9VVTT98I/1807.html:text/html;Full Text PDF:/Users/evandrake/Zotero/storage/SZJFP8WB/Munkhdalai and Trischler - 2018 - Metalearning with Hebbian Fast Weights.pdf:application/pdf},
	keywords = {neuro science / brain inspired},
	number = {{arXiv}:1807.05076},
	publisher = {{arXiv}},
	title = {Metalearning with Hebbian Fast Weights},
	url = {http://arxiv.org/abs/1807.05076},
	urldate = {2023-09-24},
	bdsk-url-1 = {http://arxiv.org/abs/1807.05076}}

@article{nawrocki_mini_2016,
	abstract = {Neuromorphic architectures are hardware systems that aim to use the principles of neural function for their basis of operation. Their goal is to harness biologically inspired concepts such as weighted connections, activation thresholds, shortand long-term potentiation, and inhibition to solve problems in distributed computation. Compared with today's methods of emulating neural function in software on conventional von Neumann hardware, neuromorphic systems provide the promise of inherently low power and fault-tolerant operation directly implemented into hardware, for application in distributed and embedded computing tasks, where the vast scaling of today's architectures does not provide a long-term solution. This mini review is intended for a general engineering audience not currently familiar with this exciting research area. It provides descriptions of some of the recent advances, including supercomputer and single-device implementations, approaches based on spiking and nonspiking neurons, machine learning hardware accelerators, and those utilizing memristive devices. Hardware implementations utilizing both conventional electronic materials and organic electronic materials are reviewed.},
	author = {Nawrocki, Robert A. and Voyles, Richard M. and Shaheen, Sean E.},
	date = {2016-10},
	date-added = {2023-11-13 10:29:55 -0800},
	date-modified = {2023-11-13 10:29:55 -0800},
	doi = {10.1109/TED.2016.2598413},
	file = {A_Mini_Review_of_Neuromorphic_Architectures_and_Implementations.pdf:/Users/evandrake/Documents/thesis/papers/A_Mini_Review_of_Neuromorphic_Architectures_and_Implementations.pdf:application/pdf},
	issn = {0018-9383, 1557-9646},
	journaltitle = {{IEEE} Transactions on Electron Devices},
	langid = {english},
	number = {10},
	pages = {3819--3829},
	shortjournal = {{IEEE} Trans. Electron Devices},
	title = {A Mini Review of Neuromorphic Architectures and Implementations},
	url = {http://ieeexplore.ieee.org/document/7549034/},
	urldate = {2023-09-18},
	volume = {63},
	bdsk-url-1 = {http://ieeexplore.ieee.org/document/7549034/},
	bdsk-url-2 = {https://doi.org/10.1109/TED.2016.2598413}}

@misc{neill_overview_2020,
	abstract = {Overparameterized networks trained to convergence have shown impressive performance in domains such as computer vision and natural language processing. Pushing state of the art on salient tasks within these domains corresponds to these models becoming larger and more difficult for machine learning practitioners to use given the increasing memory and storage requirements, not to mention the larger carbon footprint. Thus, in recent years there has been a resurgence in model compression techniques, particularly for deep convolutional neural networks and self-attention based networks such as the Transformer.},
	author = {Neill, James O'},
	date = {2020-08-01},
	date-added = {2023-11-13 10:29:55 -0800},
	date-modified = {2023-11-13 10:29:55 -0800},
	eprint = {2006.03669 [cs, stat]},
	eprinttype = {arxiv},
	file = {O'neill_survey_neural_net_compression.pdf.pdf:/Users/evandrake/Documents/thesis/papers/O'neill_survey_neural_net_compression.pdf.pdf:application/pdf},
	langid = {english},
	number = {{arXiv}:2006.03669},
	publisher = {{arXiv}},
	title = {An Overview of Neural Network Compression},
	url = {http://arxiv.org/abs/2006.03669},
	urldate = {2023-09-18},
	bdsk-url-1 = {http://arxiv.org/abs/2006.03669}}

@misc{nichol_first-order_2018,
	abstract = {This paper considers meta-learning problems, where there is a distribution of tasks, and we would like to obtain an agent that performs well (i.e., learns quickly) when presented with a previously unseen task sampled from this distribution. We analyze a family of algorithms for learning a parameter initialization that can be fine-tuned quickly on a new task, using only first-order derivatives for the meta-learning updates. This family includes and generalizes first-order {MAML}, an approximation to {MAML} obtained by ignoring second-order derivatives. It also includes Reptile, a new algorithm that we introduce here, which works by repeatedly sampling a task, training on it, and moving the initialization towards the trained weights on that task. We expand on the results from Finn et al. showing that first-order meta-learning algorithms perform well on some well-established benchmarks for few-shot classification, and we provide theoretical analysis aimed at understanding why these algorithms work.},
	author = {Nichol, Alex and Achiam, Joshua and Schulman, John},
	date = {2018-10-22},
	date-added = {2023-11-13 10:29:55 -0800},
	date-modified = {2023-11-13 10:29:55 -0800},
	eprint = {1803.02999 [cs]},
	eprinttype = {arxiv},
	file = {arXiv.org Snapshot:/Users/evandrake/Zotero/storage/YNYNZHBE/1803.html:text/html;Full Text PDF:/Users/evandrake/Zotero/storage/M2H7NDGE/Nichol et al. - 2018 - On First-Order Meta-Learning Algorithms.pdf:application/pdf},
	number = {{arXiv}:1803.02999},
	publisher = {{arXiv}},
	title = {On First-Order Meta-Learning Algorithms},
	url = {http://arxiv.org/abs/1803.02999},
	urldate = {2023-09-20},
	bdsk-url-1 = {http://arxiv.org/abs/1803.02999}}

@article{perlis_languages_1985,
	abstract = {It is argued that a proper treatment of cognitive notions such as beliefs and concepts should allow broad and consistent expression of syntax and semantics, and that this in turn depends on self-reference. A theory of quotation and unquotation is presented to this end that appears to make unnecessary the usual hierarchical and non-first-order constructions for these notions. In the current paper (part I) the underlying theory is presented; a sequel will treat in more detail the applications to cognition.},
	author = {Perlis, Donald},
	date = {1985-03},
	date-added = {2023-11-13 10:29:55 -0800},
	date-modified = {2023-11-13 10:29:55 -0800},
	doi = {10.1016/0004-3702(85)90075-X},
	file = {Perlis - 1985 - Languages with self-reference I Foundations.pdf:/Users/evandrake/Zotero/storage/5JBW6JVH/Perlis - 1985 - Languages with self-reference I Foundations.pdf:application/pdf},
	issn = {00043702},
	journaltitle = {Artificial Intelligence},
	langid = {english},
	number = {3},
	pages = {301--322},
	shortjournal = {Artificial Intelligence},
	shorttitle = {Languages with self-reference I},
	title = {Languages with self-reference I: Foundations},
	url = {https://linkinghub.elsevier.com/retrieve/pii/000437028590075X},
	urldate = {2023-10-11},
	volume = {25},
	bdsk-url-1 = {https://linkinghub.elsevier.com/retrieve/pii/000437028590075X},
	bdsk-url-2 = {https://doi.org/10.1016/0004-3702(85)90075-X}}

@misc{polu_generative_2020,
	abstract = {We explore the application of transformer-based language models to automated theorem proving. This work is motivated by the possibility that a major limitation of automated theorem provers compared to humans -- the generation of original mathematical terms -- might be addressable via generation from language models. We present an automated prover and proof assistant, {GPT}-f, for the Metamath formalization language, and analyze its performance. {GPT}-f found new short proofs that were accepted into the main Metamath library, which is to our knowledge, the first time a deep-learning based system has contributed proofs that were adopted by a formal mathematics community.},
	author = {Polu, Stanislas and Sutskever, Ilya},
	date = {2020-09-07},
	date-added = {2023-11-13 10:29:55 -0800},
	date-modified = {2023-11-13 10:29:55 -0800},
	eprint = {2009.03393 [cs, stat]},
	eprinttype = {arxiv},
	file = {arXiv.org Snapshot:/Users/evandrake/Zotero/storage/TQXUC7IQ/2009.html:text/html;Full Text PDF:/Users/evandrake/Zotero/storage/FGG395VW/Polu and Sutskever - 2020 - Generative Language Modeling for Automated Theorem.pdf:application/pdf},
	number = {{arXiv}:2009.03393},
	publisher = {{arXiv}},
	title = {Generative Language Modeling for Automated Theorem Proving},
	url = {http://arxiv.org/abs/2009.03393},
	urldate = {2023-09-18},
	bdsk-url-1 = {http://arxiv.org/abs/2009.03393}}

@misc{schlag_learning_2021,
	abstract = {Humans can quickly associate stimuli to solve problems in novel contexts. Our novel neural network model learns state representations of facts that can be composed to perform such associative inference. To this end, we augment the {LSTM} model with an associative memory, dubbed Fast Weight Memory ({FWM}). Through differentiable operations at every step of a given input sequence, the {LSTM} updates and maintains compositional associations stored in the rapidly changing {FWM} weights. Our model is trained end-to-end by gradient descent and yields excellent performance on compositional language reasoning problems, meta-reinforcement-learning for {POMDPs}, and small-scale word-level language modelling.},
	author = {Schlag, Imanol and Munkhdalai, Tsendsuren and Schmidhuber, J{\"u}rgen},
	date = {2021-02-23},
	date-added = {2023-11-13 10:29:55 -0800},
	date-modified = {2023-11-13 10:29:55 -0800},
	eprint = {2011.07831 [cs]},
	eprinttype = {arxiv},
	file = {arXiv.org Snapshot:/Users/evandrake/Zotero/storage/47D827BH/2011.html:text/html;Full Text PDF:/Users/evandrake/Zotero/storage/U6LE455W/Schlag et al. - 2021 - Learning Associative Inference Using Fast Weight M.pdf:application/pdf},
	keywords = {adaptive learning},
	number = {{arXiv}:2011.07831},
	publisher = {{arXiv}},
	title = {Learning Associative Inference Using Fast Weight Memory},
	url = {http://arxiv.org/abs/2011.07831},
	urldate = {2023-09-24},
	bdsk-url-1 = {http://arxiv.org/abs/2011.07831}}

@inproceedings{schlag_linear_2021,
	abstract = {We show the formal equivalence of linearised self-attention mechanisms and fast weight controllers from the early '90s, where a slow neural net learns by gradient descent to program the fast weights of another net through sequences of elementary programming instructions which are additive outer products of self-invented activation patterns (today called keys and values). Such Fast Weight Programmers ({FWPs}) learn to manipulate the contents of a finite memory and dynamically interact with it. We infer a memory capacity limitation of recent linearised softmax attention variants, and replace the purely additive outer products by a delta rule-like programming instruction, such that the {FWP} can more easily learn to correct the current mapping from keys to values. The {FWP} also learns to compute dynamically changing learning rates. We also propose a new kernel function to linearise attention which balances simplicity and effectiveness. We conduct experiments on synthetic retrieval problems as well as standard machine translation and language modelling tasks which demonstrate the benefits of our methods.},
	author = {Schlag, Imanol and Irie, Kazuki and Schmidhuber, J{\"u}rgen},
	booktitle = {Proceedings of the 38th International Conference on Machine Learning},
	date = {2021-07-01},
	date-added = {2023-11-13 10:29:55 -0800},
	date-modified = {2023-11-13 10:29:55 -0800},
	eventtitle = {International Conference on Machine Learning},
	file = {Full Text PDF:/Users/evandrake/Zotero/storage/CLBZ9BV9/Schlag et al. - 2021 - Linear Transformers Are Secretly Fast Weight Progr.pdf:application/pdf},
	keywords = {fast weight programmer, adaptive learning},
	langid = {english},
	note = {{ISSN}: 2640-3498},
	pages = {9355--9366},
	publisher = {{PMLR}},
	title = {Linear Transformers Are Secretly Fast Weight Programmers},
	url = {https://proceedings.mlr.press/v139/schlag21a.html},
	urldate = {2023-09-24},
	bdsk-url-1 = {https://proceedings.mlr.press/v139/schlag21a.html}}

@inproceedings{schmidhuber_self-referential_1993,
	abstract = {Weight modi cations in traditional neural nets are computed by hard-wired algorithms. Without exception, all previous weight change algorithms have many speci c limitations. Is it (in principle) possible to overcome limitations of hard-wired algorithms by allowing neural nets to run and improve their own weight change algorithms? This paper constructively demonstrates that the answer (in principle) is `yes'. I derive an initial gradientbased sequence learning algorithm for a `self-referential' recurrent network that can `speak' about its own weight matrix in terms of activations. It uses some of its input and output units for observing its own errors and for explicitly analyzing and modifying its own weight matrix, including those parts of the weight matrix responsible for analyzing and modifying the weight matrix. The result is the rst `introspective' neural net with explicit potential control over all of its own adaptive parameters. A disadvantage of the algorithm is its high computational complexity per time step which is independent of the sequence length and equals O(nconnlognconn), where nconn is the number of connections. Another disadvantage is the high number of local minima of the unusually complex error surface. The purpose of this paper, however, is not to come up with the most e cient `introspective' or `self-referential' weight change algorithm, but to show that such algorithms are possible at all.},
	annotation = {Schmidhuber compares the meta learning style of humans to that of an introspective self modifying network. networks are temporally aware specialized input and output information contain the data for how a matrix would know how to change and how it should change respectively. algorithm: an address is given to each connection of the network. providing the network with output units for addressing all of its own connections (including the connections for addressing other connections) by means of time-varying activation patterns providing input units whose activation's become the weights of the connections currently addressed by the network provide output units whose time-varying activation's serve to quickly change the weights of connections addressed by the network. Schmidhuber gives an abstract definition of a possible implementation of his {SRWM}.This paper is referentially useful as the origin point of {SRWM} notation and syntax.},
	author = {Schmidhuber, J.},
	booktitle = {{ICANN} '93},
	date = {1993},
	date-added = {2023-11-13 10:29:55 -0800},
	date-modified = {2023-11-13 10:29:55 -0800},
	doi = {10.1007/978-1-4471-2063-6_107},
	editor = {Gielen, Stan and Kappen, Bert},
	file = {document.pdf:/Users/evandrake/Zotero/storage/EQMS5QBG/document.pdf:application/pdf},
	isbn = {978-3-540-19839-0 978-1-4471-2063-6},
	keywords = {self-reference, self modification, fast weight programmer, adaptive learning},
	langid = {english},
	location = {London},
	pages = {446--450},
	publisher = {Springer London},
	title = {A `Self-Referential' Weight Matrix},
	url = {https://mediatum.ub.tum.de/doc/814784/document.pdf},
	urldate = {2023-09-18},
	bdsk-url-1 = {https://mediatum.ub.tum.de/doc/814784/document.pdf},
	bdsk-url-2 = {https://doi.org/10.1007/978-1-4471-2063-6_107}}

@misc{schmidhuber_godel_2006,
	abstract = {We present the first class of mathematically rigorous, general, fully self-referential, self-improving, optimally efficient problem solvers. Inspired by Kurt Goedel's celebrated self-referential formulas (1931), such a problem solver rewrites any part of its own code as soon as it has found a proof that the rewrite is useful, where the problem-dependent utility function and the hardware and the entire initial code are described by axioms encoded in an initial proof searcher which is also part of the initial code. The searcher systematically and efficiently tests computable proof techniques (programs whose outputs are proofs) until it finds a provably useful, computable self-rewrite. We show that such a self-rewrite is globally optimal - no local maxima! - since the code first had to prove that it is not useful to continue the proof search for alternative self-rewrites. Unlike previous non-self-referential methods based on hardwired proof searchers, ours not only boasts an optimal order of complexity but can optimally reduce any slowdowns hidden by the O()-notation, provided the utility of such speed-ups is provable at all.},
	annotation = {The Godel Machine is something of an {AGI} (artificial general intelligence). The general frame work, works on the premise that one can compare two supposedly sub-optimal problems and glean which is closer to optimality. The machine itself can theoretically be given any goal and improve in operating on that goal endlessly. The Godel machine will introspectively reflect and build theories that will eventually become proofs (given that a proof is possible). These proofs are the comparison between a state f and its successive state f'. If the machine can prove f' {\textgreater} f then it will whole sale replace f with f'. The Godel machine is built around a sequence of proofing mechanisms allowing it to trivialize its proofing procedure. This paper does not fit into my three pillars of meta learning however this is obviously a type of meta learning algorithm. This paper serves to guide my investigation into introspective proof based meta learning.},
	author = {Schmidhuber, Juergen},
	date = {2006-12-17},
	date-added = {2023-11-13 10:29:55 -0800},
	date-modified = {2023-11-13 10:29:55 -0800},
	eprint = {cs/0309048},
	eprinttype = {arxiv},
	file = {arXiv.org Snapshot:/Users/evandrake/Zotero/storage/DJE2U6PA/0309048.html:text/html;Full Text PDF:/Users/evandrake/Zotero/storage/DQKBH3RG/Schmidhuber - 2006 - Goedel Machines Self-Referential Universal Proble.pdf:application/pdf;Godel machine annotated:/Users/evandrake/Documents/Thesis/thesis_sup/papers/Godel machine annotated.pdf:application/pdf},
	keywords = {self-reference, self modification, proof theory, introspective},
	number = {{arXiv}:cs/0309048},
	publisher = {{arXiv}},
	shorttitle = {Godel Machines},
	title = {Godel Machines: Self-Referential Universal Problem Solvers Making Provably Optimal Self-Improvements},
	url = {http://arxiv.org/abs/cs/0309048},
	urldate = {2023-10-01},
	bdsk-url-1 = {http://arxiv.org/abs/cs/0309048}}

@misc{schmidhuber_godel_2006-1,
	abstract = {We present the first class of mathematically rigorous, general, fully self-referential, self-improving, optimally efficient problem solvers. Inspired by Kurt Goedel's celebrated self-referential formulas (1931), such a problem solver rewrites any part of its own code as soon as it has found a proof that the rewrite is useful, where the problem-dependent utility function and the hardware and the entire initial code are described by axioms encoded in an initial proof searcher which is also part of the initial code. The searcher systematically and efficiently tests computable proof techniques (programs whose outputs are proofs) until it finds a provably useful, computable self-rewrite. We show that such a self-rewrite is globally optimal - no local maxima! - since the code first had to prove that it is not useful to continue the proof search for alternative self-rewrites. Unlike previous non-self-referential methods based on hardwired proof searchers, ours not only boasts an optimal order of complexity but can optimally reduce any slowdowns hidden by the O()-notation, provided the utility of such speed-ups is provable at all.},
	author = {Schmidhuber, Juergen},
	date = {2006-12-17},
	date-added = {2023-11-13 10:29:55 -0800},
	date-modified = {2023-11-13 10:29:55 -0800},
	eprint = {cs.LO/0309048},
	eprinttype = {arxiv},
	file = {Godel machine poster:/Users/evandrake/Zotero/storage/HLBKW4BR/Godel machine poster.pdf:application/pdf},
	number = {{arXiv}:cs.{LO}/0309048},
	publisher = {{arXiv}},
	shorttitle = {Goedel Machines},
	title = {Godel machine poster},
	url = {http://arxiv.org/abs/cs/0309048},
	urldate = {2023-10-01},
	bdsk-url-1 = {http://arxiv.org/abs/cs/0309048}}

@incollection{schmidhuber_1998,
	abstract = {A learner's modifiable components are called its policy. An algorithm that modifies the policy is a learning algorithm. If the learning algorithm has modifiable components represented as part of the policy, then we speak of a self-modifying policy ({SMP}). {SMPs} can modify the way they modify themselves etc. They are of interest in situations where the initial learning algorithm itself can be improved by experience --- this is what we call ``learning to learn''. How can we force some (stochastic) {SMP} to trigger better and better self-modifications? The success-story algorithm ({SSA}) addresses this question in a lifelong reinforcement learning context. During the learner's life-time, {SSA} is occasionally called at times computed according to {SMP} itself. {SSA} uses backtracking to undo those {SMP}-generated {SMP}-modifications that have not been empirically observed to trigger lifelong reward accelerations (measured up until the current {SSA} call --- this evaluates the long-term effects of {SMP}-modifications setting the stage for later {SMP}-modifications). {SMP}-modifications that survive {SSA} represent a lifelong success history. Until the next {SSA} call, they build the basis for additional {SMP}-modifications. Solely by self-modifications our {SMP}/{SSA}-based learners solve a complex task in a partially observable environment ({POE}) whose state space is far bigger than most reported in the {POE} literature.},
	author = {Schmidhuber, J{\"u}rgen and Zhao, Jieyu and Schraudolph, Nicol N},
	booktitle = {Learning to \{Learn\}},
	date = {1998},
	date-added = {2023-11-13 10:29:55 -0800},
	date-modified = {2023-11-13 10:29:55 -0800},
	doi = {10.1007/978-1-4615-5529-2_12},
	editor = {Thrun, Sebastian and Pratt, Lorien},
	file = {PDF:/Users/evandrake/Zotero/storage/Q2QPK2UU/Schmidhuber et al. - 1998 - Reinforcement Learning with Self-Modifying Policie.pdf:application/pdf},
	isbn = {978-1-4613-7527-2 978-1-4615-5529-2},
	keywords = {self modification},
	location = {Boston, {MA}},
	pages = {293--309},
	publisher = {Springer {US}},
	title = {Reinforcement \{Learning\} with \{Self\}-\{Modifying\} \{Policies\}},
	url = {http://link.springer.com/10.1007/978-1-4615-5529-2_12},
	urldate = {2023-09-20},
	bdsk-url-1 = {http://link.springer.com/10.1007/978-1-4615-5529-2_12},
	bdsk-url-2 = {https://doi.org/10.1007/978-1-4615-5529-2_12}}

@incollection{schmidhuber_gomachines_2005,
	abstract = {The growing literature on consciousness does not provide a formal demonstration of the usefulness of consciousness. Here we point out that the recently formulated Godel machines may provide just such a technical justification. They are the first mathematically rigorous, general, fully self-referential, self-improving, optimally efficient problem solvers, ``conscious'' or ``self-aware'' in the sense that their entire behavior is open to introspection, and modifiable. A Godel machine is a computer that rewrites any part of its own initial code as soon as it finds a proof that the rewrite is useful, where the problem-dependent utility function, the hardware, and the entire initial code are described by axioms encoded in an initial asymptotically optimal proof searcher which is also part of the initial code. This type of total self-reference is precisely the reason for the Godel machine's optimality as a general problem solver: any self-rewrite is globally optimal---no local maxima!---since the code first had to prove that it is not useful to continue the proof search for alternative self-rewrites.},
	author = {Schmidhuber, J{\"u}rgen},
	booktitle = {Adaptive Agents and Multi-Agent Systems {II}},
	date = {2005},
	date-added = {2023-11-13 10:29:55 -0800},
	date-modified = {2023-11-13 10:29:55 -0800},
	doi = {10.1007/978-3-540-32274-0_1},
	editor = {Kudenko, Daniel and Kazakov, Dimitar and Alonso, Eduardo},
	editorb = {Hutchison, David and Kanade, Takeo and Kittler, Josef and Kleinberg, Jon M. and Mattern, Friedemann and Mitchell, John C. and Naor, Moni and Nierstrasz, Oscar and Pandu Rangan, C. and Steffen, Bernhard and Sudan, Madhu and Terzopoulos, Demetri and Tygar, Dough and Vardi, Moshe Y. and Weikum, Gerhard},
	editorbtype = {redactor},
	file = {Schmidhuber - 2005 - G{\"o}del Machines Towards a Technical Justification .pdf:/Users/evandrake/Zotero/storage/GRTEL3A2/Schmidhuber - 2005 - G{\"o}del Machines Towards a Technical Justification .pdf:application/pdf},
	isbn = {978-3-540-25260-3 978-3-540-32274-0},
	langid = {english},
	location = {Berlin, Heidelberg},
	note = {Series Title: Lecture Notes in Computer Science},
	pages = {1--23},
	publisher = {Springer Berlin Heidelberg},
	shorttitle = {G{\"o}del Machines},
	title = {G{\"o}del Machines: Towards a Technical Justification of Consciousness},
	url = {http://link.springer.com/10.1007/978-3-540-32274-0_1},
	urldate = {2023-09-20},
	volume = {3394},
	bdsk-url-1 = {http://link.springer.com/10.1007/978-3-540-32274-0_1},
	bdsk-url-2 = {https://doi.org/10.1007/978-3-540-32274-0_1}}

@inproceedings{schmidhuber_introspectivenetwork_1993,
	author = {Schmidhuber, J{\"u}rgen},
	booktitle = {1993 third international conference on artificial neural networks},
	date = {1993},
	date-added = {2023-11-13 10:29:55 -0800},
	date-modified = {2023-11-13 10:29:55 -0800},
	file = {document.pdf:/Users/evandrake/Zotero/storage/HYI2Y9SJ/document.pdf:application/pdf},
	keywords = {introspective},
	pages = {191--194},
	publisher = {{IET}},
	title = {An'introspective'network that can learn to run its own weight change algorithm}}

@article{schmidhuber_learning_1992,
	author = {Schmidhuber, J{\"u}rgen},
	date = {1992},
	date-added = {2023-11-13 10:29:55 -0800},
	date-modified = {2023-11-13 10:29:55 -0800},
	file = {document.pdf:/Users/evandrake/Zotero/storage/6LIVELQY/document.pdf:application/pdf},
	journaltitle = {Neural Computation},
	note = {Publisher: {MIT} Press},
	number = {2},
	pages = {234--242},
	title = {Learning complex, extended sequences using the principle of history compression},
	volume = {4}}

@article{schmidhuber_learning_1992-1,
	author = {Schmidhuber, J{\"u}rgen},
	date = {1992},
	date-added = {2023-11-13 10:29:55 -0800},
	date-modified = {2023-11-13 10:29:55 -0800},
	file = {document.pdf:/Users/evandrake/Zotero/storage/EMDRXL7L/document.pdf:application/pdf},
	journaltitle = {Neural Computation},
	note = {Publisher: {MIT} Press One Rogers Street, Cambridge, {MA} 02142-1209, {USA} journals-info {\ldots}},
	number = {1},
	pages = {131--139},
	title = {Learning to control fast-weight memories: An alternative to dynamic recurrent networks},
	volume = {4}}

@article{schmidhuber_ultimate_2009,
	abstract = {All life is problem solving,'' said Popper. To deal with arbitrary problems in arbitrary environments, an ultimate cognitive agent should use its limited hardware in the ``best'' and ``most efficient'' possible way. Can we formally nail down this informal statement, and derive a mathematically rigorous blueprint of ultimate cognition? Yes, we can, using Kurt Godel's celebrated self-reference trick of 1931 in a new way. Godel exhibited the limits of mathematics and computation by creating a formula that speaks about itself, claiming to be unprovable by an algorithmic theorem prover: either the formula is true but unprovable, or math itself is flawed in an algorithmic sense. Here we describe an agent-controlling program that speaks about itself, ready to rewrite itself in arbitrary fashion once it has found a proof that the rewrite is useful according to a user-defined utility function. Any such a rewrite is necessarily globally optimal---no local maxima!---since this proof necessarily must have demonstrated the uselessness of continuing the proof search for even better rewrites. Our self-referential program will optimally speed up its proof searcher and other program parts, but only if the speed up's utility is indeed provable---even ultimate cognition has limits of the Godelian kind.},
	author = {Schmidhuber, J{\"u}rgen},
	date = {2009-06},
	date-added = {2023-11-13 10:29:55 -0800},
	date-modified = {2023-11-13 10:29:55 -0800},
	doi = {10.1007/s12559-009-9014-y},
	file = {Schmidhuber - 2009 - Ultimate Cognition {\`a} la G{\"o}del.pdf:/Users/evandrake/Zotero/storage/P2VUZHIP/Schmidhuber - 2009 - Ultimate Cognition {\`a} la G{\"o}del.pdf:application/pdf},
	issn = {1866-9956, 1866-9964},
	journaltitle = {Cognitive Computation},
	langid = {english},
	number = {2},
	pages = {177--193},
	shortjournal = {Cogn Comput},
	title = {Ultimate Cognition {\`a} la G{\"o}del},
	url = {http://link.springer.com/10.1007/s12559-009-9014-y},
	urldate = {2023-09-20},
	volume = {1},
	bdsk-url-1 = {http://link.springer.com/10.1007/s12559-009-9014-y},
	bdsk-url-2 = {https://doi.org/10.1007/s12559-009-9014-y}}

@incollection{steunebrink_towards_2012,
	annotation = {This paper describes the Godel machine in respect to its implementation. There is a good dissection of a Godel machine describing its parts both pictorially and inscribed. There is a description of the intended scheduling system of a Godel machine which works to describe the implementation of its parts. The paper also gives a representation of a Godel machine in raw lambda calculus clearly defining the perceived implementation. This paper assists both in my understanding of Godel machines and in my pursuit to describe the implementation of one with more fine grained detail.},
	author = {Steunebrink, Bas R. and Schmidhuber, {J{\~A}}rgen},
	booktitle = {Theoretical Foundations of Artificial General Intelligence},
	date = {2012},
	date-added = {2023-11-13 10:29:55 -0800},
	date-modified = {2023-11-13 10:29:55 -0800},
	doi = {10.2991/978-94-91216-62-6_10},
	editor = {Wang, Pei and Goertzel, Ben},
	file = {Steunebrink and Schmidhuber - 2012 - Towards an Actual G{\"o}del Machine Implementation a .pdf:/Users/evandrake/Zotero/storage/3JQC8FIX/Steunebrink and Schmidhuber - 2012 - Towards an Actual G{\"o}del Machine Implementation a .pdf:application/pdf},
	isbn = {978-94-91216-61-9 978-94-91216-62-6},
	langid = {english},
	location = {Paris},
	note = {Series Title: Atlantis Thinking Machines},
	pages = {173--195},
	publisher = {Atlantis Press},
	shorttitle = {Towards an Actual G{\"o}del Machine Implementation},
	title = {Towards an Actual G{\"o}del Machine Implementation: a Lesson in Self-Reflective Systems},
	url = {https://www.atlantis-press.com/doi/10.2991/978-94-91216-62-6_10},
	urldate = {2023-10-01},
	volume = {4},
	bdsk-url-1 = {https://www.atlantis-press.com/doi/10.2991/978-94-91216-62-6_10},
	bdsk-url-2 = {https://doi.org/10.2991/978-94-91216-62-6_10}}

@article{sutcliffe_evaluating_2001,
	annotation = {This serves as my guide to automated theorem proving and related fields.},
	author = {Sutcliffe, Geoff and Suttner, Christian},
	date = {2001-09},
	date-added = {2023-11-13 10:29:55 -0800},
	date-modified = {2023-11-13 10:29:55 -0800},
	doi = {10.1016/S0004-3702(01)00113-8},
	file = {Submitted Version:/Users/evandrake/Zotero/storage/ERGHRXC2/Sutcliffe and Suttner - 2001 - Evaluating general purpose automated theorem provi.pdf:application/pdf},
	issn = {00043702},
	journaltitle = {Artificial Intelligence},
	langid = {english},
	number = {1},
	pages = {39--54},
	shortjournal = {Artificial Intelligence},
	title = {Evaluating general purpose automated theorem proving systems},
	url = {https://linkinghub.elsevier.com/retrieve/pii/S0004370201001138},
	urldate = {2023-10-25},
	volume = {131},
	bdsk-url-1 = {https://linkinghub.elsevier.com/retrieve/pii/S0004370201001138},
	bdsk-url-2 = {https://doi.org/10.1016/S0004-3702(01)00113-8}}

@article{tran_generalization_2023,
	abstract = {Face forgery generating algorithms that produce a range of manipulated videos/images have developed quickly. Consequently, this causes an increase in the production of fake information, making it difficult to identify. Because facial manipulation technologies raise severe concerns, face forgery detection is gaining increasing attention in the area of computer vision. In real-world applications, face forgery detection systems frequently encounter and perform poorly in unseen domains, due to poor generalization. In this paper, we propose a deepfake detection method based on meta-learning called Meta Deepfake Detection ({MDD}). The goal of the model is to develop a generalized model capable of directly solving new unseen domains without the need for model updates. The {MDD} algorithm establishes various weights for facial images from various domains. Specifically, {MDD} uses meta-weight learning to shift information from the source domains to the target domains with meta-optimization steps, which aims for the model to generate effective representations of the source and target domains. We build multi-domain sets using meta splitting strategy to create a meta-train set and meta-test set. Based on these sets, the model determines the gradient descent and obtains backpropagation. The inner and outer loop gradients were aggregated to update the model to enhance generalization. By introducing pair-attention loss and average-center alignment loss, the detection capabilities of the system were substantially enhanced. In addition, we used some evaluation benchmarks established from several popular deepfake datasets to compare the generalization of our proposal in several baselines and assess its effectiveness.},
	author = {Tran, Van-Nhan and Kwon, Seong-Geun and Lee, Suk-Hwan and Le, Hoanh-Su and Kwon, Ki-Ryong},
	date = {2023},
	date-added = {2023-11-13 10:29:55 -0800},
	date-modified = {2023-11-13 10:29:55 -0800},
	doi = {10.1109/ACCESS.2022.3232290},
	file = {Tran et al. - 2023 - Generalization of Forgery Detection With Meta Deep.pdf:/Users/evandrake/Zotero/storage/FVZRK2KF/Tran et al. - 2023 - Generalization of Forgery Detection With Meta Deep.pdf:application/pdf},
	issn = {2169-3536},
	journaltitle = {{IEEE} Access},
	langid = {english},
	pages = {535--546},
	shortjournal = {{IEEE} Access},
	title = {Generalization of Forgery Detection With Meta Deepfake Detection Model},
	url = {https://ieeexplore.ieee.org/document/9999180/},
	urldate = {2023-10-02},
	volume = {11},
	bdsk-url-1 = {https://ieeexplore.ieee.org/document/9999180/},
	bdsk-url-2 = {https://doi.org/10.1109/ACCESS.2022.3232290}}

@misc{whalen_holophrasm_2016,
	abstract = {I propose a system for Automated Theorem Proving in higher order logic using deep learning and eschewing hand-constructed features. Holophrasm exploits the formalism of the Metamath language and explores partial proof trees using a neural-network-augmented bandit algorithm and a sequence-to-sequence model for action enumeration. The system proves 14\% of its test theorems from Metamath's set.mm module.},
	author = {Whalen, Daniel},
	date = {2016-08-09},
	date-added = {2023-11-13 10:29:55 -0800},
	date-modified = {2023-11-13 10:29:55 -0800},
	eprint = {1608.02644 [cs]},
	eprinttype = {arxiv},
	file = {arXiv.org Snapshot:/Users/evandrake/Zotero/storage/CCAE4JQL/1608.html:text/html;Full Text PDF:/Users/evandrake/Zotero/storage/CYVTPGIX/Whalen - 2016 - Holophrasm a neural Automated Theorem Prover for .pdf:application/pdf},
	number = {{arXiv}:1608.02644},
	publisher = {{arXiv}},
	shorttitle = {Holophrasm},
	title = {Holophrasm: a neural Automated Theorem Prover for higher-order logic},
	url = {http://arxiv.org/abs/1608.02644},
	urldate = {2023-09-18},
	bdsk-url-1 = {http://arxiv.org/abs/1608.02644}}

@article{zou_breaking_2021,
	abstract = {The ``memory wall'' problem or so-called von Neumann bottleneck limits the efficiency of conventional computer architectures, which move data from memory to {CPU} for computation; these architectures cannot meet the demands of the emerging memory-intensive applications. Processing-in-memory ({PIM}) has been proposed as a promising solution to break the von Neumann bottleneck by minimizing data movement between memory hierarchies. This study focuses on prior art of architecture level {DRAM} {PIM} technologies and their implementation. The key challenges and mainstream solutions of {PIM} are summarized and introduced. The relative limitations of {PIM} simulation are discussed, as well as four conventional {PIM} simulators. Finally, research directions and perspectives are proposed for future development.},
	author = {Zou, Xingqi and Xu, Sheng and Chen, Xiaoming and Yan, Liang and Han, Yinhe},
	date = {2021-06},
	date-added = {2023-11-13 10:29:55 -0800},
	date-modified = {2023-11-13 10:29:55 -0800},
	doi = {10.1007/s11432-020-3227-1},
	file = {Xingqi, breaking von Neumann bottleneck.pdf:/Users/evandrake/Documents/thesis/papers/Xingqi, breaking von Neumann bottleneck.pdf:application/pdf},
	issn = {1674-733X, 1869-1919},
	journaltitle = {Science China Information Sciences},
	langid = {english},
	number = {6},
	pages = {160404},
	shortjournal = {Sci. China Inf. Sci.},
	shorttitle = {Breaking the von Neumann bottleneck},
	title = {Breaking the von Neumann bottleneck: architecture-level processing-in-memory technology},
	url = {https://link.springer.com/10.1007/s11432-020-3227-1},
	urldate = {2023-09-18},
	volume = {64},
	bdsk-url-1 = {https://link.springer.com/10.1007/s11432-020-3227-1},
	bdsk-url-2 = {https://doi.org/10.1007/s11432-020-3227-1}}

@collection{goertzel_artificial_2007,
	date = {2007},
	date-added = {2023-11-13 10:29:55 -0800},
	date-modified = {2023-11-13 10:29:55 -0800},
	editor = {Goertzel, Ben and Pennachin, Cassio},
	file = {Goertzel and Pennachin - 2007 - Artificial general intelligence.pdf:/Users/evandrake/Zotero/storage/C2GGHIMZ/Goertzel and Pennachin - 2007 - Artificial general intelligence.pdf:application/pdf},
	isbn = {978-3-540-23733-4},
	langid = {english},
	location = {Berlin ; New York},
	pagetotal = {508},
	publisher = {Springer},
	series = {Cognitive technologies},
	title = {Artificial general intelligence}}

@online{noauthor_alonzo_nodate,
	date-added = {2023-11-13 10:29:55 -0800},
	date-modified = {2023-12-06 16:17:10 -0800},
	file = {Snapshot:/Users/evandrake/Zotero/storage/WWDTNAK2/supplementD.html:text/html},
	title = {Alonzo Church {\textgreater} D. The $\Lambda$-Calculus and Type Theory (Stanford Encyclopedia of Philosophy)},
	url = {https://plato.stanford.edu/entries/church/supplementD.html#D2ChurSimpTheoType},
	urldate = {2023-10-09},
	bdsk-url-1 = {https://plato.stanford.edu/entries/church/supplementD.html#D2ChurSimpTheoType}}

@article{nasteski_overview_2017,
	abstract = {In the last decade a large number of supervised learning methods have been introduced in the field of the machine learning. Supervised learning became an area for a lot of research activity in machine learning. Many of the supervised learning techniques have found application in their processing and analyzing variety of data. One of the main characteristics is that the supervised learning has the ability of annotated training data. The so called labels are class labels in the classification process. There is a variety of algorithms that are used in the supervised learning methods. This paper summarizes the fundamental aspects of couple of supervised methods. The main goal and contribution of this review paper is to present the overview of machine learning and provide machine learning techniques.},
	author = {Nasteski, Vladimir},
	date = {2017-12-15},
	date-added = {2023-11-05 16:54:02 -0800},
	date-modified = {2023-11-05 16:54:02 -0800},
	doi = {10.20544/HORIZONS.B.04.1.17.P05},
	file = {Full Text PDF:/Users/evandrake/Zotero/storage/T53DQ6Z7/Nasteski - 2017 - An overview of the supervised machine learning met.pdf:application/pdf},
	journaltitle = {{HORIZONS}.B},
	pages = {51--62},
	shortjournal = {{HORIZONS}.B},
	title = {An overview of the supervised machine learning methods},
	volume = {4},
	bdsk-url-1 = {https://doi.org/10.20544/HORIZONS.B.04.1.17.P05}}

@misc{vanschoren_meta-learning_2018,
	abstract = {Meta-learning, or learning to learn, is the science of systematically observing how different machine learning approaches perform on a wide range of learning tasks, and then learning from this experience, or meta-data, to learn new tasks much faster than otherwise possible. Not only does this dramatically speed up and improve the design of machine learning pipelines or neural architectures, it also allows us to replace hand-engineered algorithms with novel approaches learned in a data-driven way. In this chapter, we provide an overview of the state of the art in this fascinating and continuously evolving field.},
	author = {Vanschoren, Joaquin},
	date = {2018-10-08},
	eprint = {1810.03548 [cs, stat]},
	eprinttype = {arxiv},
	file = {arXiv.org Snapshot:/Users/evandrake/Zotero/storage/HDBSHQDQ/1810.html:text/html;Full Text PDF:/Users/evandrake/Zotero/storage/YUNFNVH6/Vanschoren - 2018 - Meta-Learning A Survey.pdf:application/pdf},
	keywords = {survey},
	number = {{arXiv}:1810.03548},
	publisher = {{arXiv}},
	shorttitle = {Meta-Learning},
	title = {Meta-Learning: A Survey},
	url = {http://arxiv.org/abs/1810.03548},
	urldate = {2023-09-20},
	bdsk-url-1 = {http://arxiv.org/abs/1810.03548}}

@misc{clark_meta-learning_2022,
	abstract = {Dynamic evaluation of language models ({LMs}) adapts model parameters at test time using gradient information from previous tokens and substantially improves {LM} performance. However, it requires over 3x more compute than standard inference. We present Fast Weight Layers ({FWLs}), a neural component that provides the benefits of dynamic evaluation much more efficiently by expressing gradient updates as linear attention. A key improvement over dynamic evaluation is that {FWLs} can also be applied at training time so the model learns to make good use of gradient updates. {FWLs} can easily be added on top of existing transformer models, require relatively little extra compute or memory to run, and significantly improve language modeling perplexity.},
	author = {Clark, Kevin and Guu, Kelvin and Chang, Ming-Wei and Pasupat, Panupong and Hinton, Geoffrey and Norouzi, Mohammad},
	date = {2022-12-05},
	eprint = {2212.02475 [cs]},
	eprinttype = {arxiv},
	file = {arXiv.org Snapshot:/Users/evandrake/Zotero/storage/ASB9HUCK/2212.html:text/html;Full Text PDF:/Users/evandrake/Zotero/storage/HVX4EFGU/Clark et al. - 2022 - Meta-Learning Fast Weight Language Models.pdf:application/pdf},
	number = {{arXiv}:2212.02475},
	publisher = {{arXiv}},
	title = {Meta-Learning Fast Weight Language Models},
	url = {http://arxiv.org/abs/2212.02475},
	urldate = {2023-09-24},
	bdsk-url-1 = {http://arxiv.org/abs/2212.02475}}

@article{symons_self-reference_1997,
	author = {Symons, Cynthia S and Johnson, Blair T},
	date = {1997},
	file = {Symons and Johnson - The Self-Reference Effect in Memory A Meta-Analys.pdf:/Users/evandrake/Zotero/storage/5RZXK4XB/Symons and Johnson - The Self-Reference Effect in Memory A Meta-Analys.pdf:application/pdf},
	journaltitle = {Psychological bulletin},
	note = {Publisher: American Psychological Association},
	number = {3},
	pages = {371},
	title = {The self-reference effect in memory: a meta-analysis.},
	volume = {121}}
